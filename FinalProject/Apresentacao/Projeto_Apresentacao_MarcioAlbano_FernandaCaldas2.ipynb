{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IA898A - Processamento Digital de Imagens\n",
    "\n",
    "## Projeto Final\n",
    "\n",
    "## Reconhecimento de Teclas de Piano enquanto são tocadas \n",
    "\n",
    "## Fernanda Caldas Correia 135685\n",
    "## Marcio Albano Hermelino Ferreira 103322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[1. Dataset](#1)  \n",
    "> [1.1.Geração dos frames originais](#11)  \n",
    "[1.2. Obtenção das áreas de interesse](#12)  \n",
    "\n",
    "[2. Algoritmo](#2)\n",
    "> [2.1. Background Subtraction](#21)  \n",
    "[2.2. Diferenças entre imagens](#22)  \n",
    "[2.3. Mapeamento do teclado](#23)  \n",
    "[2.4. Mapeamento dos caracteres](#24)\n",
    "    \n",
    "[3. Reconhecimento](#3)  \n",
    "> [3.1. Notas do teclado](#31)  \n",
    "[3.2. Caracteres](#32)\n",
    "\n",
    "[4. Resultados](#4)\n",
    "> [4.1. Análise qualitativa dos resultados](#41)  \n",
    "[4.2. Análise quantitativa dos resultados](#42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 1. Dataset\n",
    "\n",
    "Inicialmente, tentamos fazer a aquisição do vídeo utilizando câmeras, mas as imagens coletadas não ficaram boas para a reconhecimento precisa das formas das teclas pretas. Os fatores que contribuiram para isso foram o posicionamento da câmera e a iluminação. Para a obtenção de resultados mais satisfatórios, foi necessário obter um vídeo que possuía uma leve inclinação ao invés da câmera estar com uma vista superior perpendicular ao plano das teclas (sendo esse último o caso da nossa tentativa de aquisição própria). Para facilitar a análise da eficácia do algoritmo, também optamos por eum vídeo em que as notas que estavam sendo tocadas fossem indicadas através da notação de cifras (C=Dó, D=Ré, E=Mi..). Dessa forma, já temos no vídeo de teste um ground truth para avaliação.\n",
    "\n",
    "O vídeo utilizado foi encontrado no YouTube: https://www.youtube.com/watch?v=WE0bFm4RPPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset\n",
    "\n",
    "## Dificuldades:\n",
    "\n",
    "- Câmera estática\n",
    "- Ângulo da captura do vídeo\n",
    "- Iluminação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src = \"original96.jpg\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 1.1.Geração dos frames originais\n",
    "\n",
    "A partir do vídeo, obtivemos um total de 14.018 frames. Os frames que são utilizados, de fato, são os primeiros 13.527 pois do 13.528 ao 14.017 são do trecho de comentários finais do canal Piano Keyz, o qual é responsável pelo vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.2. Obtenção das áreas de interesse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "O nosso objetivo é reconhecer no teclado apenas o movimento das teclas enquanto estão sendo pressionadas. Para isso, isolamos a região de cima do mesmo (em verde, dos pixels da linha 675 à linha 775) pois é a que tem menos incidência de aparição da mão. A mão só atrapalharia o reconhecimento.\n",
    "\n",
    "Além dessa região do teclado, isolamos, para analisar separadamente, o trecho (em vermelho, dos pixels da linha 610 à linha 660) que contém os caracteres das notas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src = \"regioes_interesse.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Algoritmo\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src = \"bld1.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A base da construção do projeto consiste em duas técnicas de processamento de imagens: o background subtraction e o reconhecimento de difenças entre imagens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.1. Background Subtraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Para evidenciar as teclas que estavam sendo tocadas, escolhemos a técnica de **background subtraction** . Ela consiste na separação do que é fundo da imagem *(background)*, ou objetos estáticos, e do que é informação de primeiro plano *(foreground)*, que corresponde aos objetos que estão se movendo em um vídeo. No nosso caso, serão reconhecidas as mudanças entre um frame de referência (momento em que não há nenhuma tecla sendo apertada) e o frame que se deseja analisar. Ao realizar a diferença, o que sobrará serão as teclas que foram apertadas.\n",
    "\n",
    "Existem métodos diversos para a implementação do background subtraction, e nós optamos por utilizar a função BackgroundSubtractorMOG2, da biblioteca OpenCV. Ela é implementada de acordo com o método proposto por Z. Zivkovic (ver referências no final do notebook), que é adaptativo e usa uma mistura de densidade de probabilidade gaussiana.\n",
    "\n",
    "Uma vez obtido cada frame a partir do background subtraction, eliminamos o ruído através do processo conhecido como *opening* (uma erosão seguida de uma dilatação) e transfomamos em imagem binária para facilitar a identificação das diferenças."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Métodos convencionais**: Diferença <br>\n",
    "    Frame de Referência - Frame de Análise = Objeto <br>\n",
    "    \n",
    "    \n",
    "<center><img src =\"BGsub_exemplo.png\" width=\"1000\" height=\"1000\" ></center>\n",
    "    \n",
    "- **Método utilizado:** <br>\n",
    "    Modelagem de Pixels como modelos mistura (*Mixture Models*) <br>\n",
    "    (OpenCV - BackgroundSubtractorMOG2) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### BackgroundSubtractorMOG2 (OpenCV)\n",
    "\n",
    "Z.Zivkovic <br>\n",
    " - \"Improved adaptive Gaussian mixture model for background subtraction\" (2004); <br> <br>\n",
    " - \"Efficient Adaptive Density Estimation per Image Pixel for the Task of Background Subtraction\" (2006) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background Subtraction + Redução de Ruído\n",
    "\n",
    "\n",
    "- Opening: Erosão seguida de Dilatação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kern(n):\n",
    "    return np.ones(n*n).reshape(n,n)\n",
    "\n",
    "cap = cv2.VideoCapture('radioactive.mp4')\n",
    "frm = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows = False)\n",
    "\n",
    "for i in range(int(frm)):\n",
    "    ret, frame = cap.read()        \n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = fgmask[675:775,:]\n",
    "    \n",
    "    opening = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kern(4))\n",
    "    ret,thresh1 = cv2.threshold(opening,210,255,cv2.THRESH_BINARY)\n",
    "    cv2.imwrite('Background Subtraction/bg_sub%d.jpg' %i, thresh1)\n",
    "        \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src =\"3.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src =\"b11.jpg\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src =\"22.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2. Diferença entre imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Essa função possui duas entradas de mesmo tamanho e tipo:  \n",
    "1. Uma imagem de base: um frame que representa o background. Ou seja, um frame preenchido apenas com zeros;\n",
    "2. A imagem que detecta o movimento. Ou seja, um frame com uma tecla pressionada. \n",
    "\n",
    "O primeiro passo é o cálculo do Structural Similarity Index (SSIM) entre as duas imagens, o qual retorna o índice de similaridades entre as imagens (um valor entre -1 e 1) e a imagem de diferenças. Depois, ocorre transformação da imagem de diferenças em uma imagem binária e, a partir dela, são obtidos os contornos das diferenças. Por fim, é obtida a localização de cada contorno (ou diferença). Como um único frame pode ter mais de uma tecla pressionada, criamos vetores paras as coordenadas do começo das diferenças e para o tamanho, em pixels, dessas diferenças.\n",
    "\n",
    "A mesma função é utilizada o reconhecimento das notas em caracteres. As entradas são:  \n",
    "1. Imagem base preenchida com zeros;  \n",
    "2. Imagem binária da região do piano que contém os caracteres.\n",
    "\n",
    "A função retorna a localização das diferenças."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src =\"cc.jpg\" width=1000 height=1000></center>\n",
    "\n",
    "Função para diferença de duas imagens.\n",
    "\n",
    "Retorno:\n",
    "\n",
    "Imagem de diferenças binarizada e contornos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def img_diff(imageA, imageB):\n",
    "    #Coordenadas do começo das diferenças\n",
    "    x = np.zeros((1,10))\n",
    "    y = np.zeros((1,10))\n",
    "    \n",
    "    #Tamanho das diferenças em pixels\n",
    "    w = np.zeros((1,10))\n",
    "    h = np.zeros((1,10))\n",
    "\n",
    "    #Cálculo do SSIM\n",
    "    (score, diff) = compare_ssim(imageA, imageB, full=True)\n",
    "    diff = (diff * 255).astype(\"uint8\")\n",
    "\n",
    "    #Threshold e detecção dos contornos\n",
    "    thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if imutils.is_cv2() else cnts[1]\n",
    "    \n",
    "    i = 0\n",
    "    for c in cnts:\n",
    "        #Localização dos contornos\n",
    "        (x_aux, y_aux, w_aux, h_aux) = cv2.boundingRect(c)\n",
    "        \n",
    "        #Limiar para que uma diferença seja considerada uma tecla pressionada\n",
    "        \n",
    "            #w_aux = 25 é o comprimento mínimo que a tecla deve ter\n",
    "        \n",
    "            #y_aux = 5 é a altura máxima que a diferença deve ter. Isso evita que parte da mão seja lida \n",
    "            #como uma tecla pressionada\n",
    "        if w_aux > 25 and y_aux < 5:\n",
    "            cv2.rectangle(imageB, (x_aux, y_aux), (x_aux + w_aux, y_aux + h_aux), (0, 255, 0), 2)\n",
    "            x[0][i] = x_aux\n",
    "            y[0][i] = y_aux\n",
    "            w[0][i] = w_aux\n",
    "            h[0][i] = h_aux\n",
    "            i += 1\n",
    "        \n",
    "    return x, y, w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.3. Mapeamento do Teclado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Utilizamos como base o primeiro frame do vídeo pois não contem nenhuma tecla pressionada. A região de interesse foi isolada e transformada em binária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('Frames Originais/original0.jpg', 0)\n",
    "img_cut = img[675:775,:]\n",
    "\n",
    "ret,thresh = cv2.threshold(img_cut,210,255,cv2.THRESH_BINARY)\n",
    "cv2.imwrite('base.jpg',thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "No entanto,a imagem de base gerada acima possui algumas regiões brancas dentro de teclas pretas e regiões pretas dentro de teclas brancas. Isso tornaria inviável o mapeamento de cada tecla branca e cada tecla preta através do algoritmo que implementamos, pois o mesmo se baseia em diferenciar regiões brancas das regiões pretas para classificar o tipo da tecla. Então editamos a imagem utilizando um software para contornar o problema. Abaixo, a imagem editada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"base.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como parâmetros de entrada, é necessário informar o número de teclas pretas e o número de teclas brancas que podem ser visualizadas na imagem. Isso torna o código adaptável para teclados com mais ou menos teclas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#altura do teclado\n",
    "r = 100\n",
    "base = cv2.imread('base.jpg', 0)\n",
    "\n",
    "#Brancas\n",
    "    #número de teclas brancas que aparecem na tela\n",
    "N = 33\n",
    "\n",
    "    #vetor que indica o início de cada tecla branca\n",
    "brancas = np.zeros((r, N))\n",
    "b_track = 0\n",
    "\n",
    "    #vetor que informa o tamanho de cada tecla branca\n",
    "brancas_tam = np.zeros((r, N))\n",
    "b_tam = 0\n",
    "\n",
    "#Pretas\n",
    "    #número de teclas brancas que aparecem na tela\n",
    "M = 25\n",
    "\n",
    "    #vetor que indica o início de cada tecla preta\n",
    "pretas = np.zeros((r, M))\n",
    "p_track = 0\n",
    "\n",
    "    #vetor que informa o tamanho de cada tecla preta\n",
    "pretas_tam = np.zeros((r, M))\n",
    "p_tam = 0\n",
    "\n",
    "row, col = base.shape\n",
    "\n",
    "#Distribuição das teclas ao longo do teclado\n",
    "teclado = np.zeros((1, M + N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for j in range(r):\n",
    "    b_track = 0\n",
    "    b_tam = 0\n",
    "    p_track = 0\n",
    "    p_tam = 0\n",
    "    t_track = 0\n",
    "    \n",
    "    for i in range(col):\n",
    "        #região preta\n",
    "        if base[j][i] < 210:\n",
    "            #se o pixel anterior for branco, significa o início de uma tecla preta ou de uma linha\n",
    "            if base[j][i-1] > 210:\n",
    "                brancas_tam[j][b_track] = b_tam\n",
    "                b_track += 1\n",
    "                b_tam = 0\n",
    "                p_aux = i\n",
    "            \n",
    "            #condição adicional pois o teclado começa com uma tecla preta\n",
    "            if i==0:\n",
    "                p_aux = i\n",
    "            \n",
    "            #incrementa o tamanho da região\n",
    "            p_tam += 1\n",
    "\n",
    "        #início de uma nova tecla branca\n",
    "        if base[j][i] > 210:\n",
    "            #diferenciação de uma tecla preta de uma linha de acordo com seu tamanho\n",
    "            if p_tam > 10:\n",
    "                #armazena o início de uma tecla preta\n",
    "                pretas[j][p_track] = p_aux\n",
    "                pretas_tam[j][p_track] = p_tam\n",
    "                p_track += 1\n",
    "                p_tam = 0\n",
    "                teclado[0][t_track] = -1\n",
    "                t_track += 1\n",
    "    \n",
    "            if base[j][i-1] < 210:\n",
    "                brancas[j][b_track] = i\n",
    "                \n",
    "                teclado[0][t_track] = 1\n",
    "                t_track += 1\n",
    "\n",
    "            b_tam += 1\n",
    "\n",
    "        if i == col - 1:\n",
    "            pretas[j][p_track] = p_aux\n",
    "            pretas_tam[j][p_track] = p_tam\n",
    "            teclado[0][t_track] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "original = cv2.imread('cv2thre.jpg')\n",
    "aux = np.copy(original)\n",
    "r, c, b = original.shape\n",
    "\n",
    "#brancas\n",
    "for i in range(N):\n",
    "    for row in range(100):\n",
    "        start = int(brancas[row][i])\n",
    "        end = start + int(brancas_tam[row][i])\n",
    "        aux[row,start:end] = (0, 255, 0)\n",
    "        teclado_brancas[row,start:end,i] = 255 \n",
    "\n",
    "    #cv2.imwrite('Mapeamento Teclado/brancas%d.jpg' %i, teclado_brancas[:,:,i])\n",
    "    #cv2.imwrite('Mapeamento Teclado/teclado_brancas%d.jpg' %i, aux)\n",
    "    aux = np.copy(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"original_brancas.gif\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "original = cv2.imread('cv2thre.jpg')\n",
    "aux = np.copy(original)\n",
    "r, c, b = original.shape\n",
    "\n",
    "#pretas\n",
    "for i in range(M):\n",
    "    for row in range(100):\n",
    "        start = int(pretas[row][i])\n",
    "        end = start + int(pretas_tam[row][i])\n",
    "        aux[row,start:end] = (0, 255, 0)\n",
    "        teclado_pretas[row,start:end,i] = 255 \n",
    "    \n",
    "    #cv2.imwrite('Mapeamento Teclado/pretas%d.jpg' %i, teclado_pretas[:,:,i])\n",
    "    #cv2.imwrite('Mapeamento Teclado/teclado_pretas%d.jpg' %i, aux)\n",
    "    aux = np.copy(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"original_pretas.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.4. Mapeamento dos Caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Isolamos a região que contém os caracters em cada frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "frm = 13527\n",
    "\n",
    "for i in range(int(frm)):\n",
    "    img = cv2.imread('Frames Originais/original%d.jpg' %i, 0)\n",
    "    img = img[610:660,:]\n",
    "    \n",
    "    ret,thresh = cv2.threshold(img,100,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    cv2.imwrite('Notas em caracteres/nota_carac%d.jpg' %i, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#função análoga a img_diff, mas para os caracteres (cifras)\n",
    "def img_diff_carac(imageA, imageB):\n",
    "    x = np.zeros((1,10))\n",
    "    y = np.zeros((1,10))\n",
    "    w = np.zeros((1,10))\n",
    "    h = np.zeros((1,10))\n",
    "\n",
    "    (score, diff) = compare_ssim(imageA, imageB, full=True)\n",
    "    diff = (diff * 255).astype(\"uint8\")\n",
    "\n",
    "    thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if imutils.is_cv2() else cnts[1]\n",
    "    \n",
    "    i = 0\n",
    "    for c in cnts:\n",
    "        (x_aux, y_aux, w_aux, h_aux) = cv2.boundingRect(c)\n",
    "        \n",
    "        if w_aux >10:\n",
    "            x[0][i] = x_aux\n",
    "            y[0][i] = y_aux\n",
    "            w[0][i] = w_aux\n",
    "            h[0][i] = h_aux\n",
    "            i += 1\n",
    "        \n",
    "    return x, y, w, h\n",
    "\n",
    "def cut_nota(img, x, w):\n",
    "    if x!=0:\n",
    "        start = int(x)\n",
    "        end = int(x) + int(w)\n",
    "        img_cut = img[:,start:end]\n",
    "        return img_cut\n",
    "    \n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A partir das imagens geradas acima, isolamos algumas notas usando a função img_diff_carac. Assim, pudemos obter os caracteres que servirão de base para o reconhecimento das notas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Designamos:  \n",
    "0 - C  \n",
    "1 - D  \n",
    "2 - E  \n",
    "3 - F  \n",
    "4 - G  \n",
    "5 - A  \n",
    "6 - B\n",
    "\n",
    "sust0 - C#  \n",
    "sust1 - D#   \n",
    "sust2 - F#  \n",
    "sust3 - G#  \n",
    "sust4 - A#\n",
    "\n",
    "No vídeo só aparecem as seguintes notas, que foram preenchidas para que notas do mesmo tipo tenham o mesmo tamanho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"D.jpg\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"E.jpg\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"A.jpg\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"B.jpg\" alt=\"Drawing\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"sust0.jpg\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"sust2.jpg\" alt=\"Drawing\"/> </td>\n",
    "<td> <img src=\"sust3.jpg\" alt=\"Drawing\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Todos os caracteres usados como base foram previamente carregados em uma matriz para facilitar o processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Molde dos caracteres\n",
    "carac = np.zeros((50, 38, 8), dtype='uint8')\n",
    "carac_sust = np.zeros((50, 65, 5), dtype='uint8')\n",
    "\n",
    "for i in range(8):\n",
    "    if i ==1 or i == 2 or i == 5 or i ==6:\n",
    "        carac[:,:,i] = cv2.imread('Notas em caracteres/Mapeamento/%d.jpg' %i, 0)\n",
    "        #cv2.imwrite('Notas em caracteres/Mapeamento/TESTE%d.jpg' %i, carac[:,:,i])\n",
    "\n",
    "for i in range(5):\n",
    "    if i ==0 or i == 2 or i == 3:\n",
    "        carac_sust[:,:,i] = cv2.imread('Notas em caracteres/Mapeamento/sust%d.jpg' %i, 0)\n",
    "        #cv2.imwrite('Notas em caracteres/Mapeamento/TESTESUST%d.jpg' %i, carac_sust[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "base = cv2.imread('Notas em caracteres/Mapeamento/base.jpg', 0)\n",
    "\n",
    "for i in range(5):\n",
    "    molde = cv2.imread('Notas em caracteres/Mapeamento/molde%d.jpg' %i, 0)\n",
    "    diff, x, y, w, h = img_diff_carac(base, molde)\n",
    "    \n",
    "    for j in range(10):\n",
    "        if x[0][j] != 0:\n",
    "            carac = cut_nota(molde, x[0][j], w[0][j])\n",
    "            cv2.imwrite('Notas em caracteres/Mapeamento/%d_%d.jpg' %(i, j), carac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "nota = np.zeros((50, 38))\n",
    "nota_sust = np.zeros((50, 65))\n",
    "\n",
    "for i in range(4):\n",
    "    img  = cv2.imread('Notas em caracteres/Mapeamento/nota%d.jpg' %i, 0)\n",
    "    c = img.shape[1]\n",
    "    aux = 38 - c\n",
    "    nota[:,aux//2: c + aux//2] = img\n",
    "    cv2.imwrite('Notas em caracteres/Mapeamento/%d.jpg' %i, nota)\n",
    "\n",
    "for i in range(3):\n",
    "    img  = cv2.imread('Notas em caracteres/Mapeamento/notasust%d.jpg' %i, 0)\n",
    "    c = img.shape[1]\n",
    "    aux = 65 - c\n",
    "    nota_sust[:,aux//2: c + aux//2] = img\n",
    "    cv2.imwrite('Notas em caracteres/Mapeamento/sust%d.jpg' %i, nota_sust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Reconhecimento\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 3.1. Notas do Teclado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "base = cv2.imread('Background Subtraction/bg_sub0.jpg', 0)\n",
    "r, c = base.shape\n",
    "#criado vetor com duas linhas para poder comparar as coisas na mesma imagem\n",
    "fill = np.zeros((2*r + 20, c), dtype='uint8')\n",
    "\n",
    "#no maximo 10 notas podem ser tocadas ao mesmo tempo\n",
    "#entao o maximo de diferenças que podem ser detectadas é 10\n",
    "\n",
    "comp = np.zeros((1, N + M, 10))\n",
    "\n",
    "notas_tecla = np.zeros((int(frm), 7, 5), dtype='uint8')\n",
    "notas_tecla_sust = np.zeros((int(frm), 5, 5), dtype='uint8')\n",
    "\n",
    "aux = np.zeros((r, c), dtype='uint8')\n",
    "\n",
    "frm = 13527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#percorrer todos frames\n",
    "for i in range(frm):\n",
    "    frame = cv2.imread('Background Subtraction/bg_sub%d.jpg' %i, 0)\n",
    "    x, y, w, h = img_diff(base, frame)\n",
    "    fill[:100,:] = frame\n",
    "    \n",
    "    for j in range(10):\n",
    "        #caso tenha encontrado diferença, preenche o polígono de branco\n",
    "        if x[0][j] != 0:\n",
    "            start = int(x[0][j])\n",
    "            end = start + int(w[0][j])\n",
    "            aux[:,start:end] = 255\n",
    "            \n",
    "            #compara a região pintada  com regiões de mesmo tamanho para todas as notas.\n",
    "            for k in range(N):\n",
    "                comp[0, k, j] = compare_ssim(aux[:,start:end], teclado_brancas[:,start:end,k])\n",
    "                \n",
    "            for k in range(N, N + M):\n",
    "                comp[0, k, j] = compare_ssim(aux[:,start:end], teclado_pretas[:,start:end,k - N])\n",
    "            #pega a nota com a maior similaridade \n",
    "            row, col, depth = np.unravel_index(np.argmax(comp, axis=None), comp.shape)\n",
    "            \n",
    "            #vetor de comparação: todas teclas brancas e depois todas as pretas\n",
    "            #para as brancas\n",
    "            if col < N:\n",
    "                o = (col+1)//7\n",
    "                nota = (col+1)%7\n",
    "                notas_tecla[i][nota][o] = 1\n",
    "            #para as pretas        \n",
    "            if col >= N:\n",
    "                o = (col-N)//5\n",
    "                nota = (col-N)%5\n",
    "                notas_tecla_sust[i][nota][o] = 1\n",
    "        \n",
    "        comp = np.zeros((1, N + M, 10))\n",
    "        aux = np.zeros((r, c), dtype='uint8')\n",
    "        \n",
    "        #testando se oa matriz de reconhecimento está funcionando\n",
    "        if x[0][j]==0 and x[0][j-1]!=0:\n",
    "            #indica a nota e qual oitava pertence  \n",
    "            white, oitava = np.nonzero(notas_tecla[i])\n",
    "            nz_white = white.shape[0]\n",
    "            \n",
    "            for s in range(nz_white):\n",
    "                #localização da tecla reconhecida\n",
    "                n = int(7*oitava[s] + white[s] - 1)\n",
    "                \n",
    "                #preenchimento com base no mapeamento de cada tecla\n",
    "                for count in range(100):\n",
    "                    start = int(brancas[count][n])\n",
    "                    end = start + int(brancas_tam[count][n])\n",
    "                    fill[120:120+count,start:end] = 255\n",
    "                    \n",
    "            nz_white = 0\n",
    "            \n",
    "            #indica a nota e qual oitava pertence        \n",
    "            black, oitava = np.nonzero(notas_tecla_sust[i])\n",
    "            nz_black = black.shape[0]\n",
    "                \n",
    "            for s in range(nz_black):\n",
    "                #localização da tecla reconhecida\n",
    "                n = int(5*oitava[s] + black[s])\n",
    "                \n",
    "                #preenchimento com base no mapeamento de cada tecla\n",
    "                for count in range(100):\n",
    "                    start = int(pretas[count][n])\n",
    "                    end = start + int(pretas_tam[count][n])\n",
    "                    fill[120:120+count,start:end] = 255\n",
    "                    \n",
    "            nz_black = 0\n",
    "            \n",
    "    cv2.imwrite('teste teclado/teste%d.jpg' %i, fill)\n",
    "    fill = np.zeros((r*2 + 20, c), dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2. Caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "base = cv2.imread('Notas em caracteres/nota_carac0.jpg', 0)\n",
    "linha, coluna = base.shape\n",
    "frm = 13556\n",
    "\n",
    "notas_carac = np.zeros((int(frm), 7), dtype='uint8')\n",
    "notas_carac_sust = np.zeros((int(frm), 5), dtype='uint8')\n",
    "\n",
    "#Divisão em oitavas\n",
    "oit = np.array([[0, 368, 768, 1162, 1562]])\n",
    "notas_carac_oit = np.zeros((int(frm), 7, 5), dtype='uint8')\n",
    "notas_carac_sust_oit = np.zeros((int(frm), 5, 5), dtype='uint8')\n",
    "\n",
    "\n",
    "fill = np.zeros((2*linha + 20, coluna), dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(int(frm)):\n",
    "    img = cv2.imread('Notas em caracteres/nota_carac%d.jpg' %i, 0)\n",
    "    fill[:50,:] = img\n",
    "    dif, x, y, w, h = img_diff_carac(base, img)\n",
    "    \n",
    "    \n",
    "    fill[:50,366:370] = 255\n",
    "    fill[:50,766:770] = 255\n",
    "    fill[:50,1160:1164] = 255\n",
    "    fill[:50,1560:1564] = 255\n",
    "    \n",
    "    ssim = np.zeros((1, 7))\n",
    "    ssim_sust = np.zeros((1, 5))\n",
    "    \n",
    "    for j in range(10):\n",
    "        if x[0][j] != 0:\n",
    "            f = np.zeros((50, 38), dtype='uint8')\n",
    "            fsust = np.zeros((50, 65), dtype='uint8')\n",
    "            nota = cut_nota(img, x[0][j], w[0][j])\n",
    "            c = nota.shape[1]\n",
    "            \n",
    "            if c < 38:\n",
    "                aux = 38 - c\n",
    "                f[:,aux//2: c + aux//2] = nota\n",
    "                \n",
    "                for k in range(7):\n",
    "                    ssim[0][k] = compare_ssim(carac[:,:,k], f)\n",
    "                    \n",
    "                row, col = np.unravel_index(np.argmax(ssim, axis=None), ssim.shape)\n",
    "                notas_carac[i][col] += 1\n",
    "                \n",
    "                o = 4\n",
    "                while o > -1:\n",
    "                    if x[0][j] > oit[0][o]:\n",
    "                        notas_carac_oit[i][col][o] = 1\n",
    "                        break\n",
    "                    o -= 1\n",
    "                        \n",
    "                white, oitava = np.nonzero(notas_carac_oit[i])\n",
    "\n",
    "                nz = white.shape[0]\n",
    "                \n",
    "                for s in range(nz):\n",
    "                    n = int(7*oitava[s] + white[s] - 1)\n",
    "                    start = int(brancas[0][n])\n",
    "                    fill[70:,start:start + 38] = carac[:,:,int(white[s])]\n",
    "            \n",
    "            if c > 38:\n",
    "                aux = 65 - c\n",
    "                fsust[:,aux//2: c + aux//2] = nota\n",
    "                \n",
    "                for k in range(5):\n",
    "                    ssim_sust[0][k] = compare_ssim(carac_sust[:,:,k], fsust)           \n",
    "\n",
    "                row, col = np.unravel_index(np.argmax(ssim_sust, axis=None), ssim_sust.shape)\n",
    "                notas_carac_sust[i][col] += 1\n",
    "                \n",
    "                o = 4\n",
    "                while o > -1:\n",
    "                    if x[0][j] > oit[0][o]:\n",
    "                        notas_carac_sust_oit[i][col][o] = 1\n",
    "                        break\n",
    "                    o -= 1\n",
    "                        \n",
    "                black, oitava = np.nonzero(notas_carac_sust_oit[i])\n",
    "\n",
    "                nz = black.shape[0]\n",
    "                \n",
    "                for s in range(nz):\n",
    "                    n = int(5*oitava[s] + black[s])\n",
    "                    start = int(pretas[0][n])\n",
    "                    fill[70:,start:start + 65] = carac_sust[:,:,int(black[s])]\n",
    "                        \n",
    "    \n",
    "    #cv2.imwrite('teste caracteres/teste%d.jpg' %i, fill)\n",
    "    #cv2.imwrite('teste caracteres/a_oitava_teste%d.jpg' %i, fill)\n",
    "    fill = np.zeros((2*linha + 20, coluna), dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Por fim, é feita a escrita do arquivo de vídeo a partir dos frames processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "height , width = fill.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "video = cv2.VideoWriter('teste_caracteres.mp4', fourcc, 10.0,(width,height))\n",
    "\n",
    "for i in range(int(frm)):\n",
    "    img = cv2.imread('teste caracteres/teste%d.jpg' %i)\n",
    "    video.write(img)\n",
    "    \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "frm = 13527\n",
    "\n",
    "height , width = fill.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "video = cv2.VideoWriter('comparacaoteclado.mp4', fourcc, 10.0,(width,height))\n",
    "\n",
    "for i in range(int(frm)):\n",
    "    img = cv2.imread('teste teclado/teste%d.jpg' %i)\n",
    "    video.write(img)\n",
    "    \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 4.1. Análise Qualitativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.2. Análise Quantitativa\n",
    "<center><img src = \"PIZZA.png\"></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "frm = 13527\n",
    "original = cv2.imread('Frames Originais/original0.jpg')\n",
    "r, c, b = original.shape\n",
    "original_brancas = np.zeros((100, c, b, N), dtype='uint8')\n",
    "original_pretas = np.zeros((100, c, b, M), dtype='uint8')\n",
    "\n",
    "for i in range(N):\n",
    "    original_brancas[:,:,:,i] = cv2.imread('Mapeamento Teclado/original_brancas%d.jpg' %i)\n",
    "    \n",
    "for i in range(M):\n",
    "    original_pretas[:,:,:,i] = cv2.imread('Mapeamento Teclado/original_pretas%d.jpg' %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "frm = 13527\n",
    "\n",
    "for i in range(frm):\n",
    "    frame = cv2.imread('Frames Originais/original%d.jpg' %i)\n",
    "    \n",
    "    white, oitava = np.nonzero(notas_tecla[i])\n",
    "    nz_white = white.shape[0]\n",
    "                \n",
    "    for s in range(nz_white):\n",
    "        n = int(7*oitava[s] + white[s] - 1)\n",
    "        \n",
    "        for count in range(100):\n",
    "            start = int(brancas[count][n])\n",
    "            end = start + int(brancas_tam[count][n])\n",
    "            frame[675:775,start:end,:] = (0, 255, 0)\n",
    "\n",
    "    nz_white = 0\n",
    "                    \n",
    "    black, oitava = np.nonzero(notas_tecla_sust[i])\n",
    "    nz_black = black.shape[0]\n",
    "                \n",
    "    for s in range(nz_black):\n",
    "        n = int(5*oitava[s] + black[s])\n",
    "        \n",
    "        for count in range(100):\n",
    "            start = int(pretas[count][n])\n",
    "            end = start + int(pretas_tam[count][n])\n",
    "            frame[675:775,start:end,:] = (0, 255, 0)\n",
    "                    \n",
    "    nz_black = 0\n",
    "    \n",
    "    cv2.imwrite('Video final/final%d.jpg' %i, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "frm = 13527\n",
    "\n",
    "frame = cv2.imread('Video final/final0.jpg')\n",
    "height , width, depth = frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "video = cv2.VideoWriter('comparacaofinal.mp4', fourcc, 10.0,(width,height))\n",
    "\n",
    "for i in range(int(frm)):\n",
    "    img = cv2.imread('Video final/final%d.jpg' %i)\n",
    "    video.write(img)\n",
    "    \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Obrigado!\n",
    "\n",
    "## Referências\n",
    "\n",
    "\n",
    "[1] ZIVKOVIC, Z. \"Improved adaptive Gaussian mixture model for background subtraction\" (2004); <br> \n",
    "[2] ZIVKOVIC, Z. \"Efficient Adaptive Density Estimation per Image Pixel for the Task of Background Subtraction\" (2006) <br>\n",
    "[3] SUTEPARUK, P. \"Detection of Piano Keys Pressed in Video\" <br>\n",
    "[4] AKBARI, M.; CHENG, H. \"claVision: Visual Automatic Piano Music Transcription\" <br> \n",
    "[5] NISBET, A.; GREEN, R. \"Capture of Dynamic Piano Performance with Depth Vision\" <br>\n",
    "[6] https://www.pyimagesearch.com/2017/06/19/image-difference-with-opencv-and-python/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
